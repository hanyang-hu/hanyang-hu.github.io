---
title: "A Note on Kolmogorov Complexity and Shannon Entropy"
excerpt: "My report for the course project in CS3236 Introduction to Information Theory."
collection: portfolio
---

This [report](./CS3236_KolmogorovComplexity.pdf) is what I have written for my course project [CS3236 Introduction to Information Theory](https://nusmods.com/courses/CS3236/introduction-to-information-theory) taught by Prof. Jonathan Scarlett during AY22/23 Sem 2. This is my first time writing reports this long in LaTex other than my maths assignments (say, those for the notoriously difficult [MA2101S Linear Algebra II (S)](https://nusmods.com/courses/MA2101S/linear-algebra-ii-s) taught by Prof. Tan Kai Meng). I burnt for many days without studying anything else just to understand those materials, which was quite fun to be honest. In general, I get to learn about Turing machines and how it is used to quantify complexity, say the Kolmogorov complexity of \\(\pi\\) is finite since there exists a program to estimate it to any precision, despite it takes infinite memory to store all the digits directly. This motivates me to take [CS3231 Theory of Computation](https://nusmods.com/courses/CS3231/theory-of-computation) in the following academic year, which also turns out to be rewarding.

**Remark.** One particularly painful memory was that some sources do not clearly state whether they are using the **self-delimiting** version of Kolmogorov complexity, which confused me when different results are presented from different sources. On another note, I did not pay much attention to the writing and disrupted the flow by using too many footnotes. This is a habit I adapted from writing my math assignments, where the emphasis is on correctness and rigor rather than on readability, as long as the grader can discern the main ideas and logic.

The next time I saw the notion of Kolmogorov complexity was quite recent, in [this paper on Keops](https://proceedings.neurips.cc/paper/2020/hash/a6292668b36ef412fa3c4102d1311a62-Abstract.html) where the author claims that KeOps essentially leverages the low Kolmogorov complexity of symbolic arrays to achieve smaller runtime and memory usage for certain types of computation. The KeOps library might be very useful in my ongoing FYP where I need to train a full exact Gaussian process (GP) model and conduct Lanczos estimation based on it during test time: when I have 100k data, matrix-vector multiplication with the kernel matrix requires way more GPU memory if using PyTorch only, and are only made available with the help of KeOps.