---
title: "Unstructured High-Dimensional Bayesian Optimization"
excerpt: "The report and presentation for my summer research project in 2024.<br/><img src='/images/eggholder_2d.png'>"
collection: portfolio
---

Links to the materials: [report](UROPS_REPORT_Hu_Hanyang_Jonathan_Scarlett.pdf), [slides](UROPS_SLIDE_Hu_Hanyang_Jonathan_Scarlett.pdf) and [GitHub repository](https://github.com/hanyang-hu/unstructured_highdim_bo).

This is a credit-bearing undergraduate research project (MA3288) under the supervision of Prof. Jonathan Scarlett during the summer of 2024. The focus was around [this paper](https://arxiv.org/abs/2402.02229) by Hvarfner et al. that proposed the **dimensionality-scaled lengthscale prior** (DSP) to counteract the increase in complexity due to increased dimensionality, which relates to the [hypercube line picking problem](https://mathworld.wolfram.com/HypercubeLinePicking.html): for two points uniformly sampled in the \\(D\\)-dimensional hypercube, the mean distance between them grows asymptotically as fast as \\(\sqrt{D}\\), and hence **data points tend to be less correlated in higher dimensions**. Therefore, the DSP approach simply **scales up** the expectation of the prior of lengthscales by a factor of \\(\sqrt{D}\\), which could also be interpreted as imposing a low-complexity assumption on the objective functions, without using any structural assumptions (e.g., locality, low-dimensional active subspaces, etc.). 

The following figure from Hvarfner et al. shows the connection between lengthscale and model complexity:
![lengthscales and complexity](./low_complexity_exp.png)


What I did during the summer was trying to improve upon DSP by considering the **unknown hyperparameter issue** for Bayesian optimization in high-dimensional settings. A common technique in the low-dimensional setting is to gradually shrink lengthscales (i.e., gradually making harder assumptions) to avoid underfitting (as illustrated below by [Berkenkamp et al.](https://jmlr.org/papers/v20/18-213.html)), such as [AR cool down](https://arxiv.org/abs/1612.03117) or [A-GP-UCB](https://jmlr.org/papers/v20/18-213.html). It seems that DSP is even more prone to underfitting since it makes a low-complexity assumption through the prior, hence lengthscale cool-down strategies might be worth trying. 
![underfitting with small lengthscale](./unknown_hyp.png)

The lengthscale cooldown methods that worked for low-dimensional problems cannot be directly applied in the high-dimensional setting when we use **automatic relevance determination** (ARD), i.e. estimate different lengthscales for different dimensions. Both AR cool down and A-GP-UCB shrink the lengthscale estimated initially, hence the relevance of each dimension never changes. In high-dimensional setting, there are more lengthscales to be estimated, and due to the **curse of dimensionality**, more data points (typically more than the number of points generated by the Sobol engine for initialization) are required to give reasonable estimates, and hence shrinking the initial lengthscale might not be a good idea:
![different in low and high dimensions](./unknown_hyp_high_dim.png)
Consequently, I tried out two "meta-strategies" that shrink the prior or posterior of lengthscales respectively. 

[TODO: describe soft Winsorization]

[TODO: write about my recent thinkings on the hypercube line thinking problem]